# 卷积神经网络人脸识别
---

### munpy教程
Numpy是Python中用于科学计算的核心库。
它提供了高性能的多维数组对象，以及相关工具。

#### Python基本数据类型

Python拥有一系列的基本数据类型，比如整型、浮点型、布尔型和字符串等

1. 布尔型
布尔逻辑用的是英语(and or not)，而不是(&&和||、！)
1. 字符串
字符串对象具有一系列属性方法，但是本身不可改变。
1. 整形
整形与浮点型不具备++/--操作
1. 浮点型
整除//  普通除法/ 与python2不同

#### Python引用类型

Python拥有引用类型包括容器与对象

1. 集合(sets)
集合是独立不同个体的无序集合。

1. 元组(Tuples)
元组是一个值的有序列表（不可改变）。
从很多方面来说，元组和列表都很相似。
和列表最重要的不同在于，元组可以在字典中用作键，
还可以作为集合的元素，而列表不行。

1. 列表(list)
Python中的数组，但是长度可变且能够包含不同类型元素
包含多种属性与方法
***列表推导***
` [x*2 for x in [0,1,2,3,4 if x %2 == 0]] `

1. 字典(dictionaries)
字典用来存储键值对，与map类似

1. 对象(object)
对象是类在堆内存中的实例化引用对象。

#### 数组(Arrays)
一个numpy数组是一个由不同数值组成的网格。
网格中的数据都是同一种数据类型，可以通过非负整型数的元组来访问。
维度的数量被称为数组的阶，
数组的大小是一个由整型数构成的元组，可以描述数组不同维度上的大小。

```
import numpy as np
b = np.array([[1,2,3],[4,5,6]])
a = np.zeros((2,2)) #全0矩阵
b = np.ones((1,2)) #全1矩阵
c = np.full((2,2), 2) #全2矩阵
d = np.eye(2) #对角矩阵
e = np.random.random((2,2)) #随机矩阵

```

1. 数组计算

np.add(x, y) 加
np.subtract(x, y) 减
np.multiply(x, y) 乘
np.divide(x, y) 除
np.dot(v, w) 矩阵乘法
x.T 矩阵转置

1. 广播Broadcasting

广播是一种强有力的机制，它让Numpy可以让不同大小的矩阵在一起进行数学计算。
我们常常会有一个小的矩阵和一个大的矩阵，
然后我们会需要用小的矩阵对大的矩阵做一些计算。

### SciPy教程
Numpy提供了高性能的多维数组，以及计算和操作数组的基本工具。
SciPy基于Numpy，提供了大量的计算和操作数组的函数，
这些函数对于不同类型的科学和工程计算非常有用。

#### 图像操作
~~~python
from scipy.misc import imread, imsave, imresize
img = imread('assets/cat.jpg')
print img.dtype, img.shape  # Prints "uint8 (400, 248, 3)"

img_tinted = img * [1, 0.95, 0.9]

# Resize the tinted image to be 300 by 300 pixels.
img_tinted = imresize(img_tinted, (300, 300))

# Write the tinted image back to disk
imsave('assets/cat_tinted.jpg', img_tinted)
~~~

### 图像分类(KNN)

L1距离：绝对值求和
L2距离：均方求和
L1和L2比较。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。
也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。
L1和L2都是在p-norm常用的特殊形式。

k-Nearest Neighbor：它的思想很简单：与其只找最相近的那1个图片的标签，
找最相似的k个图片的标签，然后让他们针对测试图片进行投票，
最后把票数最高的标签作为对测试图片的预测。

***决不能使用测试集来进行调优，防止过拟合***

交叉验证：训练集数量较小（因此验证集的数量更小），
人们会使用一种被称为交叉验证的方法。

### 线性分类

y=wx+b

#### 评分函数


#### 损失函数 ？？？？
softmax分类器：
交叉熵损失
数据预处理：

### 反向传播

反向传播是利用链式法则递归计算表达式的梯度的方法。


### 神经网络

激活函数
sigmoid函数y=1/(1+e-x)
tanh函数
ReLU函数max(0,x)
Leaky ReLU函数max(0.01x,x)
Maxout函数

#### 过拟合

防止神经网络的过拟合有很多方法
（L2正则化，dropout和输入噪音等）

数据预处理
PCA和白化（Whitening）是另一种预处理形式。
在这种处理中，先对数据进行零中心化处理，
然后计算协方差矩阵，它展示了数据中的相关性结构。

梯度下降
随机梯度下降
动量方法
Nesterov动量方法

学习率衰减
随步数衰减：每进行几个周期就根据一些因素降低学习率。
典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。
这些数值的设定是严重依赖具体问题和模型的选择的。
在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，
每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。

指数衰减：数学公式是a=a*e^{-kt}，其中a,k是超参数，
t是迭代次数.

1/t衰减：数学公式是a=a/(1+kt)，
其中\alpha_0,k是超参数，t是迭代次数。

### CNN


#### 卷积层

参数共享
局部链接
ReLU

#### 采样层pooling


#### 全连接层

---
神经元：
  y=wx+b
  求解w、b(初始化问题)
非线性激活函数(模拟非线性关系s)：
  归一化(防止过饱和)
  sigmoid：
    Logistic/softmax
    tanh
    梯度弥散
BP算法/反向传播
  链式法则
  损失函数
  梯度下降
    随机梯度下降
    动量法
    学习率


初始化/梯度下降(学习率)/数据集大小(影响精度)
超参数

交叉验证
\测试集(验证集只能最后验证，防止记忆发生过拟合)




过拟合(正则化解决过拟合问题)
dropout(随机失活一般神经元)
数据预处理
归一化
正则化
  小权重意味着网络的行为不会因为我们随意更改了一些
  输入而改变太多。这使得它不容易学习到数据中局部噪声
  超参数(L2范数)
标准化

卷积神经网络(特征映射)
  数据输入层/  Input layer
    去均值
    归一化
    PCA/白化
  卷积计算层 / CONV layer
    局部关联
    参数共享
    权重共享
    边缘填充
  ReLU 激励层 / ReLU layer
    非线性映射
  池化层 / Pooling layer
    压缩数据
    最大池化提取特征值
    防止边缘填充和稀疏矩阵的影响
  全连接层 / FC layer
    最后全连接计算

